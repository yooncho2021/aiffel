{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coated-rebate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL 라이브러리 import 완료!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob\n",
    "\n",
    "print(\"PIL 라이브러리 import 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "small-christopher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202 images to be resized.\n",
      "202 images resized.\n",
      "가위 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")\n",
    "    \n",
    "    print(len(images), \"images to be resized.\")\n",
    "    \n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    print(len(images), \"images resized.\")\n",
    "\n",
    "image_dir_path=os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"가위 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "juvenile-raising",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 images to be resized.\n",
      "201 images resized.\n",
      "바위 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")\n",
    "    \n",
    "    print(len(images), \"images to be resized.\")\n",
    "    \n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    print(len(images), \"images resized.\")\n",
    "\n",
    "image_dir_path=os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"바위 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "upset-intranet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 images to be resized.\n",
      "201 images resized.\n",
      "보자기 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")\n",
    "    \n",
    "    print(len(images), \"images to be resized.\")\n",
    "    \n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    print(len(images), \"images resized.\")\n",
    "\n",
    "image_dir_path=os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"보자기 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affiliated-destruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 604 입니다.\n",
      "x_train shape: (604, 28, 28, 3)\n",
      "y_train shape: (604,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=604):  \n",
    "    \n",
    "    img_size=28\n",
    "    color=3\n",
    "    \n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=0   \n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=1   \n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=2   \n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0   \n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "described-order",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0])\n",
    "print('라벨:', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "quiet-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suffering-football",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  7\n"
     ]
    }
   ],
   "source": [
    "#모델 만들기\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(256, (3,3), activation='relu', input_shape=(28,28,3))) #이미지 채널 수 바꿔야함!!!!\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(512, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(512, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "print('Model에 추가된 Layer 개수: ', len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "wicked-survey",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 256)       7168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               6554112   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 7,742,979\n",
      "Trainable params: 7,742,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "electronic-explanation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(604, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "preliminary-alabama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19/19 [==============================] - 6s 129ms/step - loss: 160.0060 - accuracy: 0.3603\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.9282 - accuracy: 0.5590\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.5017 - accuracy: 0.7734\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.2724 - accuracy: 0.9001\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.1880 - accuracy: 0.9220\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.1129 - accuracy: 0.9732\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.0770 - accuracy: 0.9740\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.0450 - accuracy: 0.9914\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.0327 - accuracy: 0.9958\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 0s 18ms/step - loss: 0.0407 - accuracy: 0.9743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9f40b9f1d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "tamil-salon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images to be resized.\n",
      "0 images resized.\n",
      "가위 테스트 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")\n",
    "    \n",
    "    print(len(images), \"images to be resized.\")\n",
    "    \n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    print(len(images), \"images resized.\")\n",
    "\n",
    "image_dir_path=os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor-samples\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"가위 테스트 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adverse-constitutional",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트데이터(x_test)의 이미지 개수는 300 입니다.\n",
      "x_test shape: (300, 28, 28, 3)\n",
      "y_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=300):  \n",
    "    \n",
    "    img_size=28\n",
    "    color=3\n",
    "    \n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor-samples/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=0   \n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock-samples/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=1   \n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper-samples/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=2   \n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"테스트데이터(x_test)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test\"\n",
    "(x_test, y_test)=load_data(image_dir_path)\n",
    "x_test_norm = x_test/255.0   \n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "purple-blood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL 라이브러리 import 완료!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob\n",
    "\n",
    "print(\"PIL 라이브러리 import 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "coral-macro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 images to be resized.\n",
      "100 images resized.\n",
      "가위 테스트 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")\n",
    "    \n",
    "    print(len(images), \"images to be resized.\")\n",
    "    \n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    print(len(images), \"images resized.\")\n",
    "\n",
    "image_dir_path=os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/scissor-samples\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"가위 테스트 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "french-student",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 images to be resized.\n",
      "100 images resized.\n",
      "바위 테스트 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")\n",
    "    \n",
    "    print(len(images), \"images to be resized.\")\n",
    "    \n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    print(len(images), \"images resized.\")\n",
    "\n",
    "image_dir_path=os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/rock-samples\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"바위 테스트 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "traditional-disease",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 images to be resized.\n",
      "100 images resized.\n",
      "보 테스트 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")\n",
    "    \n",
    "    print(len(images), \"images to be resized.\")\n",
    "    \n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    print(len(images), \"images resized.\")\n",
    "\n",
    "image_dir_path=os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/paper-samples\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"보 테스트 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mental-attempt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_test)의 이미지 개수는 300 입니다.\n",
      "x_test shape: (300, 28, 28, 3)\n",
      "y_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=300):  \n",
    "    \n",
    "    img_size=28\n",
    "    color=3\n",
    "    \n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor-samples/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=0   \n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock-samples/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=1   \n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper-samples/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    \n",
    "        labels[idx]=2   \n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_test)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test\"\n",
    "(x_test, y_test)=load_data(image_dir_path)\n",
    "x_test_norm = x_test/255.0   \n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caroline-suspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_test[0])\n",
    "print('라벨:', y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "radio-floor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  7\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(256, (3,3), activation='relu', input_shape=(28,28,3))) #이미지 채널 수 바꿔야함!!!!\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(512, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(512, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "print('Model에 추가된 Layer 개수: ', len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "possible-transaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 256)       7168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               6554112   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 7,742,979\n",
      "Trainable params: 7,742,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fatty-ferry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "right-signal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 3s 244ms/step - loss: 244.2775 - accuracy: 0.3195\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 1.8154 - accuracy: 0.3516\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 1.0188 - accuracy: 0.4900\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.7257 - accuracy: 0.7320\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.4781 - accuracy: 0.7850\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.2762 - accuracy: 0.9004\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.1291 - accuracy: 0.9753\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.0603 - accuracy: 0.9816\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.0397 - accuracy: 0.9871\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.0399 - accuracy: 0.9845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9fc7ff2090>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_test, y_test, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "democratic-consultation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 0.0073 - accuracy: 1.0000\n",
      "test_loss: 0.007313027512282133 \n",
      "test_accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "endless-adelaide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.predict() 결과 :  [9.9998498e-01 1.5065080e-05 6.3703376e-11]\n",
      "model이 추론한 가장 가능성이 높은 결과 :  0\n",
      "실제 데이터의 라벨 :  0\n"
     ]
    }
   ],
   "source": [
    "predicted_result = model.predict(x_test)  \n",
    "predicted_labels = np.argmax(predicted_result, axis=1)\n",
    "\n",
    "idx=0  \n",
    "print('model.predict() 결과 : ', predicted_result[idx])\n",
    "print('model이 추론한 가장 가능성이 높은 결과 : ', predicted_labels[idx])\n",
    "print('실제 데이터의 라벨 : ', y_test[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "considerable-lightning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                25632     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 31,299\n",
      "Trainable params: 31,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 16.6970 - accuracy: 0.3529\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.1095 - accuracy: 0.5871\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 1.7307 - accuracy: 0.6843\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.6590 - accuracy: 0.7910\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4742 - accuracy: 0.8512\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.6109 - accuracy: 0.8015\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.7014 - accuracy: 0.7870\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2294 - accuracy: 0.9079\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2126 - accuracy: 0.9318\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.2043 - accuracy: 0.9343\n",
      "10/10 - 0s - loss: 1.1519 - accuracy: 0.6433\n",
      "test_loss: 1.1519348621368408 \n",
      "test_accuracy: 0.6433333158493042\n"
     ]
    }
   ],
   "source": [
    "n_channel_1=16\n",
    "n_channel_2=32\n",
    "n_dense=32\n",
    "n_train_epoch=10\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=n_train_epoch)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-sierra",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
